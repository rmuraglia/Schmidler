% detmeth.tex

% detailed methods for the appendix of the duke thesis

\chapter{Detailed Methods}

This appendix contains detailed method descriptions and parameter values required to reproduce the work shown in this thesis. \\
Code is available at \url{https://github.com/rmuraglia/Schmidler}.

\section{Exhaustive search} % (fold)
\label{sec:exhaustive_search}

\subsection{Dijkstra's algorithm} % (fold)
\label{sub:dijkstra_s_algorithm}

Dijkstra's algorithm is a dynamic programming algorithm for finding minimum cost paths through graphs.

\begin{algorithm}
\caption{Dijkstra's algorithm}
\begin{algorithmic}[1]
    \STATE Initialize queue $Q$ containing all nodes of the graph.
    \STATE Set node attributes as: $node.dist=\infty, node.prev=$NA, $\forall$ nodes.
    \STATE For node corresponding to initial state, set $node.dist=0$.
    \WHILE{$Q$ not empty}
        \STATE Pop node with minimum $node.dist$ from $Q$. Set as $u$.
        \STATE Set $V$ as the list of neighbor nodes to $u$.
        \FOR {$v$ in $V$}
            \STATE Calculate the distance to $v$ via $u$ as $v.alt = u.dist + d(u,v)$.
            \IF {$v.alt < v.dist$}
            \STATE This is a new shortest path to $v$.
            \STATE Set $v.dist = v.alt$ and $v.prev = u$
            \ENDIF
        \ENDFOR
    \ENDWHILE
\end{algorithmic}
\end{algorithm}

Technically, because we are only interested in the shortest path between a given pair of nodes, we can terminate the algorithm when the selected $u$ in line 5 is the target node.
To reconstruct a solution path, we can simply trace our way backwards. 
Starting with the target node, we simply need to note the $node.prev$ value, and keep on iterating backwards until we reach the initial node, with a $node.prev$ value of NA.

For figure \ref{uniuni}, the initial and target distributions were $\mathcal{N}(0,1)$ and $\mathcal{N}(5,1)$ distributions, respectively.
More specifically, they were defined with the potential function $U(x) = \frac{(x-\mu)^2}{2\sigma^2}$, and the unnormalized density $q(x) = \exp(-\beta U(x))$.
The initial and target state had $(\mu, \sigma)$ pairs $(0,1), (5,1)$, respectively.
Intermediate distributions were defined by $\lambda$-scaling, as shown in equation \eqref{lambdascaling}.

For figure \ref{unibi}, the initial distribution was the same standard normal, but the target distribution was a bimodal distribution, defined by a quartic potential $U(x) = (ax^4 - bx^2)/(\sigma^2c)$. Here, $a=1, b=81, c=150, \sigma=1$.

For both cases, the distance metric was the total variation distance, as defined by Roberts and Rosenthal\cite{roberts2004general}: $d_{TV}(p_1, p_2)=E_1 \left [\min \left (1,\frac{p_2(x)}{p_1(x)} \right ) \right ]$. 
The search parameters used were 1000 draws per node, and a $move.jump$ coefficient of 4.

For figure \ref{varbar-paramviews}, the distributions were defined as they were for figures \ref{uniuni} and \ref{unibi}. Here temperature varied in the grid instead of $\sigma$. $\sigma$ was held constant at 1, but $\beta$, the inverse temperature varied according to the grid coordinates.
The distance metric was the asymptotic variance of the BAR estimator, as defined in equation \eqref{eq:varbar}.
In both cases, there were 1000 draws per node and a $move.jump$ coefficient of 5.

% subsection dijkstra_s_algorithm (end)

\subsection{Fixed path length search} % (fold)
\label{sub:fixed_path_length_search}

In equation \eqref{eq:varbar}, there is a leading $1/N$ variance reduction term based on the number of samples drawn at each node. 
As a result, when 1000 draws are sampled from each state, paths with more edges benefit from this leading term more.
In other words, depending on path length, some paths can be thought of as more computationally costly.
In figure \ref{fig:kshortest} we compare equal computation paths by fixing the total number of samples drawn per path.
To do this, we carry out a dynamic programming search, similar to Dijkstra's algorithm, which returns the shortest path of specified length. 

\begin{algorithm}
\caption{Fixed path length search}
\begin{algorithmic}[1]
    \STATE Initialize $C$ matrix values to $\infty$, where $C[i,j]$ represents the cost of the cheapest path from the initial state to state $i$ in $j$ steps.
    \STATE Initialize $C[init,1]=0$.
    \STATE Initialize $P$ matrix values to NA, where $P[i,j]$ represents the previous state corresponding to the move that gave rise to $C[i,j]$.
    \FOR {$k$ in $2:maxlength$}
        \STATE Set $Q^\prime$ as the states with a complete path in $k-1$ steps ($C[,k-1]\neq \infty$)
        \STATE Set $Q$ as the neighbors of $Q^\prime$. These are the nodes to be queried.
        \FOR {$q$ in $Q$}
            \STATE set $V$ as the list of neighbor nodes to $q$.
            \FOR {$v$ in $V$}
                \STATE Calculate the distance to $q$ via $v$ in $k$ steps as $q.alt = C[v,k-1] + d(v,q)$.
                \IF {$q.alt < C[q,k]$}
                    \STATE This is a new shorter path to $q$.
                    \STATE Set $C[q,k] = q.alt$ and $P[q,k] = v$.
                \ENDIF
            \ENDFOR
        \ENDFOR
    \ENDFOR
\end{algorithmic}
\end{algorithm}

At the completion of the algorithm, we can then directly read off the unscaled path costs, constrained to a path length, which can then be corrected to account for the number of edges in the path.
Obtaining the solution path is done in the same way as for Dijkstra's algorithm. 
We simply need to trace back values in the $P$ matrix.

For the harmonic wells, the end point distributions were the same as previously. Each state was sampled from 5000 times to obtain the unscaled edge cost, which was then scaled with an effective sample size. The $move.jump$ coefficient was 4.

For the unimodal to bimodal case, each state was sampled from 1000 times, and the $move.jump$ coefficient was 10. The initial normal distribution state was the same as before, but the bimodal distribution now has coefficients: $a=0.5, b=14, c=64$.
% subsection fixed_path_length_search (end)
% section exhaustive_search (end)

\section{Sequential Monte Carlo} % (fold)
\label{sec:sequential_monte_carlo}

In this document, we presented four sequential Monte Carlo based free energy estimators: annealed importance sampling (AIS, equivalent to Jarzynski's method), sequential BAR (seqBAR, or sBAR), Crooks (an estimator which follows from the Crooks fluctuation theorem), and pairwise Crooks (pCrooks).
Each method can be decomposed into two steps: nonequilibrium sampling and ratio estimation.
% Keep in mind that there are many different ways of describing these algorithms, depending on if you would prefer to think in terms of work (energy differences) or density ratios. 
% Here I present one 
When transitions are carried out as symmetric Metropolis moves, the nonequilibrium sampling protocol is as follows:

\begin{algorithm}
\caption{A nonequilibrium sampler with Metropolis transitions}
\begin{algorithmic}[1]
    \STATE Define a series of distributions $\{p_1, ..., p_n\}$
    \STATE Generate $N$ independent draws $\{x_1^{(i)}\}$ from $p_1$
    \STATE Initialize weights: $\{w_1^{(i)}\}=1/N$
    \FOR {$j=2$ to $n$}
        \STATE Calculate incremental weights: $\{\tilde{w}_{j-1}\} = q_j(x_{j-1})/q_{j-1}(x_{j-1})$
        \IF {RESAMPLE}
            \STATE Generate $\{ \tilde{x}_{j-1} \}$ by sampling $\{ x_{j-1} \}$ with replacement with probabilities $\{\tilde{w}_{j-1}\}$
            \STATE Reset weights: $\{ w_{j-1} \} = 1/N$
        \ELSE
            \STATE Set $\{ \tilde{x}_{j-1} \}$ to $\{ x_{j-1} \}$
        \ENDIF
        \STATE Normalize weights: $w_j = (w_{j-1}*\tilde{w}_{j-1})/ (\sum w_{j-1}*\tilde{w}_{j-1})$
        \STATE Carry out a nonequilibrium transition
        \FOR {$i=1$ to $N$}
            \STATE Propagate each particle to the next distribution
            \STATE $x_j^{(i)} \sim Metropolis(\tilde{x}_{j-1}^{(i)}, q_j)$
        \ENDFOR
    \ENDFOR
\end{algorithmic}
\end{algorithm}

In this work, the Metropolis transitions consisted of accept/reject steps for trial configurations generated by drawing from a Gaussian centered on the current state with scale 0.5.
The ``RESAMPLE'' flag is always set to ``FALSE'' for AIS, Crooks and pCrooks. 
For seqBAR, the flag is always set to ``TRUE.'' 
In general usage, we would resample when the effective sample size criterion dips below a prespecified threshold, such as suggested by Del Moral, Doucet and Jasra \cite{del2006sequential}.
For AIS, we only run the sampler in one trajectory generation direction. 
For seqBAR, we may run it either only on direction, or we can combine information from both propagation directions.
For Crooks and pCrooks, the sampler must be run in both directions ($1$ to $n$ and $n$ to $1$).

For AIS, the ratio estimation step is simple.
First, calculate full trajectory particle weights by multiplying incremental weights for each particle, then obtain a ratio estimate by taking the mean of the full trajectory particle weights.

For seqBAR, because we resample at each step, our draws approximate the equilibrium distributions. 
We can then directly apply BAR estimation to these draws as described in equation \eqref{bridgesam} and Meng and Schilling\cite{meng2002warp}.

As previously noted, there is significant overlap in these methods. 
For the Crooks estimator, we directly apply equation (19) from Crooks\cite{crooks2000path} with the same type of iterative scheme as we do for BAR.
Converting between work and ratios of densities, or free energies and ratios of normalizing constants is trivial and left to the reader.

Lastly, the pCrooks estimator is nearly identical to the BAR estimator. 
When precomputing $l$ and $s$ terms, as described in Meng and Schilling's equation, we must take care to use the correct draws, such that when weighted, they approximate the desired distribution.
In general, to approximate the ``forward'' draws for the $j$th distribution, use $w_{j} x_{j-1}$, and for the ``reverse'' draws for the $j$th distribution, use $w_{j} x_{j+1}$.
For the first and last distribution edge cases, simply use $x_1$ and $x_n$ weighted by uninformative weight of $1/N$.
In plain terms, we use the unweighted draws specified in the previous sentence to calculate $l$ and $s$, then use the normalized weights as a leading correction term in the sum.

\subsection{Specific parameter values} % (fold)
\label{sub:specific_parameter_values}

To create the figures, the following run parameters for the SMC algorithms were used:

For figure \ref{fig:sBARvAIS}, the initial distribution was $\mathcal{N}(0,1)$, and the target distribution was a bimodal with parameters $a=0.5, b=14, c=64$.
All trajectories were propagated in the normal to bimodal direction.
The temperature was held fixed.

For figure \ref{fig:AIS-pathcompare}, the initial and target distributions were $\mathcal{N}(0,1)$ and $\mathcal{N}(5,1)$ distributions, respectively.
Each Metropolis transition has 25 accept/reject steps.
Each path had 21 $\lambda$ points, which were spaced uniformly along the $\lambda$ coordinate. 
Their temperature values were chosen to lie along a half-ellipse defined as: $T(\lambda)=\sqrt{h^2 - h^2/c^2 * (\lambda-c)^2} +1$, where $c$ indicates the center of the ellipse (here $c=0.5$), and $h$ indicates the maximum height of the path.

For figure \ref{fig:pcrooks}, the initial distribution was $\mathcal{N}(0,1)$, and the target distribution was a t-distribution with one degree of freedom (a Cauchy distribution).
Each algorithm was run with six distributions in the path, 600 particles per ratio estimate, and ten mixing steps per transition.
For Crooks, pCrooks and sBAR, trajectories were evenly split between forward and reverse propagation. 
For AIS all trajectories were run in the forward direction.
The temperature was held fixed for all paths.

% subsection specific_parameter_values (end)

% section sequential_monte_carlo (end)

\section{Reinforcement learning} % (fold)
\label{sec:reinforcement_learning}

\subsection{AIS bandit} % (fold)
\label{sub:ais_bandit}

For figure \ref{fig:aisbandit}, each AIS run had 200 particles, with 25 mixing steps per transition.
The bandit strategy was selecting between three paths, each with 26 distributions per path. 
The three paths were defined with the same semi-ellipse scheme as above, with maximum heights of 0, 2 and 4.
All particles were propagated from the $\mathcal{N}(0,1)$ to the $\mathcal{N}(5,1)$ distribution.
Each strategy ran 90 separate ratio estimates. 
Ten were allocated to each path to get an estimate of the path quality (the variance of the ratio estimates). 
The remaining 60 were allocated as described in the main body of the text.
% subsection ais_bandit (end)

\subsection{Q-learning} % (fold)
\label{sub:q_learning}

Here we present complete Q-learning pseudocode to illustrate our method combined with pCrooks and stochastic edge weight evaluation.

\begin{algorithm}
\caption{Q-learning pseudocode}
\begin{algorithmic}[1]
    \STATE Initialize all $Q(s,a)$ values to 1
    \WHILE {Stop condition not met}
        \STATE Run a Q-learning episode
        \STATE Propagate particles forward
        \STATE Set $s$ to initial distribution
        \STATE Generate draws from $s$
        \WHILE {$s \neq$ target state}
            \STATE Select an outbound action ($a$) from state ($s$)
            \STATE Propagate particles from $s$ to resultant state of action $a$
            \STATE Obtain SMC weights associated with transition
            \STATE Set $s$ as resultant state of action $a$
        \ENDWHILE
        \STATE Propagate particles backwards along same path
        \STATE Set $s$ to target distribution
        \STATE Generate draws from $s$
        \WHILE {$s \neq$ initial state}
            \STATE Propagate particles from $s$ to previous state in path
            \STATE Obtain SMC weights
            \STATE Set $s$ as previous state in path
        \ENDWHILE
        \STATE Calculate pCrooks ratio and var(ratio) based on forward draws, forward weights, reverse draws and reverse weights
        \STATE Add var(ratio) to reward table entry for each (state, action) pair in path
        \STATE Update Q map according to equation (5.1)
    \ENDWHILE
\end{algorithmic}
\end{algorithm}

The action selection move and stopping criteria vary, depending on the type of Q-learning run.
Different options are discussed below.
To get the current best-path, we simply trace the minimum outbound $Q$ value, starting from the initial state.

For figure \ref{fig:ql-summary}, additional parameter values not discussed in the main text are as follows. 
The initial and target distributions are $\mathcal{N}(0,0.7^2)$ and $\mathcal{N}(5,0.7^2)$, respectively. 
The $move.jump$ coefficient for the search was 1.
Each search agent calculated ratios with pCrooks with 100 particles.
At each transition, there were 25 Metropolis mixing steps.
The Q map values were initialized to a value of 1.
Next state selection was based on two factors: an $\epsilon$ value representing the impact of randomness on the search, and the $Q$ values for potential next moves.
With probability $\epsilon$, we simply selected the minimum $Q$ move, otherwise we selected from the next possible moves randomly.
We set $\epsilon$ to vary linearly, starting from a value of 0, and ending with a value of 1, once 85\% of the learning episodes were complete.
The stopping criteria was when 600 Q-learning episodes had elapsed, for this fixed duration Q-learning.

For figure \ref{fig:ql-convergence}, here we present additional details on the convergence method.
We run three separate Q-learning chains, which maintain their own independent $Q$ and reward maps. 
For a $min.episode$ number of episodes (here 30), they run independently, searching randomly ($\epsilon=0$).
Following this initial learning period, after every episode, we assess the convergence of each chain. \\
To do this, we construct a composite estimate, based on a mean of the three chain ratio estimates, weighted inversely by their variances.
We then construct a window around this composite estimate as $estimate \pm estimate*conv.tol$ (here $conv.tol=0.025$).
For each chain, we construct an interval as the chain estimate $\pm$ the chain standard deviation.
If the entire chain interval is contained within the composite window, we claim that chain to be converged.
When all three chains are converged, the Q-learning algorithm is deemed converged. \\
After the initial learning period, the $\epsilon$ values are tuned starting from a value of 0.3, according to each chain's converged status.
If the chain is converged, we increase $\epsilon$ by $\Delta \epsilon$ (here set to 0.005), with an $\epsilon$ cap of 1.
If the chain is not converged, we aim for a steady state $\epsilon$ value of 0.8.
One way to accomplish this stochastically is provided in the code.
% subsection q_learning (end)

% section reinforcement_learning (end)